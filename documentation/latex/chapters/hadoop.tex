\section{Hadoop Implementation}
\subsection{Introduction}
In this section is briefly described some features done in the code for speeding up the execution of the code introduced in the pseudocode presented in the previous chapter. After that, the performance evaluation of the computation is reported.


\subsection{First Phase: Nodes Counting}
The counting is done making use of a combiner with the same code of the reducer, this helps up to speed up the processing at the reducer with a simple sum of three values (i.e., equal to the number of mappers used). The value computed is stored in a temporary file that we will erased after being read and stored inside the \textit{Configuration} object.

\subsection{Second Phase: Graph construction}
Parsing is a job that doesn't not need to create a conglomerate value, but it creates records which, each of them, is treated as a standalone piece of information, thus using multiple reducers is possible. We decide to use three reducers since we have three working nodes.

\subsection{Third Phase: PageRank Estimation}
As in the \textit{Graph construction} phase multiple reducers are used for accelerating the computation. Thus, from the first iteration to the last one this MapReduce computation takes multiple inputs and produces multiple outputs.

\subsection{Forth Phase: Sorting}
Since the previous phase constructs multiple outputs, the sorting phase has to take multiple inputs. Here a single reducer is used in order to take advantage of the automatic sorting done over the keys by Hadoop. Keys are in this case the pagerank value of each page, and are passed by the mapper to the reducer as \textbf{DoubleWritable} objects.

\subsection{Performance}
For measuring the performance we considered three files with a different number of nodes:
\begin{itemize}
	\item \textbf{wiki-micro.txt}: number of nodes 2427. Most of the nodes points to sites that are not present in the initial set.
	\item \textbf{dataset5.txt}: number of nodes 5000. This dataset is synthetic dataset created using the same structure of \textit{wiki-micro.txt}. Each node has a random value of outgoing edges between 0 and 10.
	\item \textbf{dataset10.txt}: number of nodes 10000. This dataset is synthetic dataset created using the same structure of \textit{wiki-micro.txt}. Each node has a random value of outgoing edges between 0 and 10.
\end{itemize}
and we test them over different numbers of iteration: 5, 10, 15.

\begin{table}[H]
\caption{Performance}
\centering
\begin{tabular}{c c c c}
\hline\hline
File & N Iter 5 & N Iter 10 & N Iter 15 \\ [0.7ex] % inserts table %heading
\hline
wiki-micro.txt&184417&314290&436651\\
dataset5.txt&186320&310445&467191 \\
dataset10.txt&206945&339101&455945 \\ [1ex]
\hline
\end{tabular}
\label{table:nonlin}
\end{table}

The values in \textbf{Table 1} are all in milliseconds.