\section{Hadoop Implementation}

\subsection{Introduction}
In this section a description of the MapReduce implementation of \textit{Page Rank} is given.
The algorithm is carried out in \textbf{three distinct steps}:
\begin{enumerate}
	\item \textit{Graph Construction} phase
	\item \textit{Page Rank Computation} phase
	\item \textit{Sorting} phase
\end{enumerate}

\noindent We have decided to represent the structure of the graph through adjacency lists, so each node (that represents a page) will keep the list of outgoing edges as status information, as well as its ranking.

\subsection{Pseudocode}
In this chapter we will report a pseudocode of the three phases, to better understand the implementation in Hadoop.
\subsubsection{First Phase: Graph Construction}
In this phase we parse the information in the input file taking the information that interests us, i.e. the title of the page and the outgoing edges. In addition to this, we take advantage of this phase to also perform the count of the graph nodes.

\begin{algorithm}[H]
	\caption{Graph Construction Mapper}\label{GraphConstructionMapper}
		\begin{algorithmic}[1]
			\Procedure{Setup}{}
				\State $\textit{counter} \gets \textit{0}$
			\EndProcedure
			\Procedure{Map}{PageId id, Page p}
					\State $\textit{title} \gets \textit{getTitle(p)}$
					\State $ outgoingEdges \gets \textit{getOutgoingEdges(p)}$
					\State $\textit{counter} \gets \textit{counter + 1}$
					\State EMIT(title, outgoingEdges)
			\EndProcedure
			\Procedure{Cleanup}{}
				\State EMIT("", \textit{counter})	
			\EndProcedure
			
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Graph Construction Reducer}\label{GraphConstructionReducer}
		\begin{algorithmic}[1]

	\Procedure{Reduce}{Title t, $[e_1, e_2, \dots]$}
		\State $\textit{initialPageRank} \gets -1 $
		\If{t == ""}
		\Comment{If these are the counters}
		\State $globalCounter \gets$ 0
		\ForAll {e \textit{in} $[e_1, e_2, \dots]$}
		\State $globalCounter \gets globalCounter + e$
		\EndFor
		\State EMIT("", globalCounter)
		\Else
		\State $edges \gets e_1$
		\State EMIT(t, \{initialPageRank, edges\})
		\EndIf
		
	\EndProcedure
		\end{algorithmic}
\end{algorithm}

\noindent In \textit{Algorithm 3} only $e_1$ is considered as edges because the title is a unique identifier of the pages, so the list of input values will always consist of a single element. Only one mapper will manage one page. This is not the case when we receive the local counters from the mappers. More information can be found in the "Implementations Details" chapter.

\subsubsection{Second Phase: PageRank Estimation}
In this section, the pagerank iteration is presented.
\noindent The number of iteration is fixed at the start of the execution. So the termination condition is the number of iterations.

\begin{algorithm}
	\caption{PageRank Computation Mapper}\label{Mapper}
		\begin{algorithmic}[1]
			
			\Procedure{Setup}{}
			\State $N \gets numberOfNodes$
			\EndProcedure
			
			\Procedure{Map}{Key k, Node n}
			\If{n.pagerank == -1}
			\Comment{First time}
			\State $n.pagerank = \frac{1}{N}$
			\EndIf
			\State EMIT(n.title, \{-1, n.outgoingEdges\}) \Comment{Pass along the graph structure}
			\State $massToSend = \frac{n.pagerank}{n.outgoingEdges.length}$
			\ForAll{outgoingEdge \textbf{in} n.outgoingEdges}
			\State EMIT(outgoingEdge, $\{massToSend, []\}$)
			\Comment{Pass PageRank mass to neighbors}
			\EndFor
			\EndProcedure
	\end{algorithmic}
\end{algorithm}
\noindent Note: \textit{outgoingEdge} is a title itself.


\begin{algorithm}[H]
	\caption{PageRank Computation Reducer}\label{Reducer}
		\begin{algorithmic}[1]
			\Procedure{Setup}{}
				\State $N \gets numberOfNodes$
				\State $damping \gets 0.8$
			\EndProcedure
		
			\Procedure{Reduce}{Title t, Nodes $[n_1, n_2, \dots]$}
			\State $s \gets 0$
			\Comment{Ingoing mass}
			\State $n \gets null$
			\ForAll {node \textbf{in} nodes}
				\If {node.pagerank == -1}
					\State $n \gets node$
				\Else
					\State $s \gets s + node.pagerank$
				\EndIf
			\EndFor
			\If{n != null}
			\Comment{If it is a node}
				\State$n.pagerank \gets  \frac{(1-D)}{N} + D*s$
				\State EMIT(t, n)
			\EndIf
			\EndProcedure
	\end{algorithmic}
\end{algorithm}



\subsubsection{Third Phase: Sorting}
The final step is sorting the webpages by decreasing rank, this is done making advantage of the sorting mechanism of MapReduce.

\begin{algorithm}[H]
	\caption{Sorting Mapper}\label{Mapper}
	\begin{algorithmic}[1]
		\Procedure{Map}{Key k, Node n}
		\State $title \gets n.title$
		\State $pagerank \gets n.pagerank$
		\State EMIT(pagerank, title)
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Sorting Reducer}\label{Reducer}
	\begin{algorithmic}[1]
		\Procedure{Reduce}{Pagerank rank, Titles $[t_1, t_2, \dots]$}
		\ForAll {title \textbf{in} titles}
		\State EMIT(title, rank)
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Implementations Details}
In these following chapters is briefly described how we have implemented the pseudocode. After that, the performance evaluation of the computation is reported.

\subsubsection{First Phase: Graph construction}
Parsing is a job that doesn't need to create a conglomerate value, but it creates records which, each of them, is treated as a standalone piece of information. We decide to use three reducers since we have three working nodes.

\noindent Initially we had thought of inserting an initial step dedicated exclusively to the node count, but later we came up with a better solution, complicating the parsing phase. In the section on performance the differences between the two approaches will be shown, showing the improvement obtained.

\noindent First we consider the Mapper, in which each record is parsed, looking for the title and all the outgoing links. So for each record we are going to emit a pair \textbf{(title, outgoing edges)}. In addition to this we must also count the number of nodes, to do this we use an \textbf{In-Mapper Combiner}, a global counter inside the Mapper. It will be initialized to 0 in setup, and incremented at each execution of the map function; during the cleanup we will transmit a single cumulative value for that Mapper, transmitting the pair \textbf{("", N)}, with N the value of the counter. As a key we use the empty string "", which cannot belong to any title and which is always placed first in lexicographic order.
Since we send two different information between Mapper and Reducer, we have decided to implement our own \textbf{partitioner}, to always send partial counters to Reducer 0 and therefore be sure to have this value in the file part-r-00000. All other keys will be split into the other reducers, using an hash function, like the default behavior.
As for the reducer, it takes care of adding the partial values of the counters to obtain N, the number of nodes; in addition it will get the title and outgoing links values of each record. The initial page rank value is set to -1, and will be calculated at the first iteration of the next step. All this information will be saved in the filesystem, to be processed in the next step. The next step will take in input all the output file containing the Node information. All this process has been schematized in \ref{fig:parsingPhase}


\begin{figure}[H]
	\includegraphics[width=\textwidth]{img/ParserSchema.png}
	\caption{Parsing phase}
	\label{fig:parsingPhase}     
\end{figure}

\subsubsection{Second Phase: PageRank Estimation}
Since the previous phase constructs multiple outputs, this phase has to take multiple inputs
As in the \textit{Graph construction} phase multiple reducers are used for accelerating the computation. Thus, from the first iteration to the last one this MapReduce computation takes multiple inputs and produces multiple outputs.
\newline Since we have to output Node-type values from the mapper, the mass must also be transmitted through this data structure. Since the Pagerank value for a certain node depends only on the amount of mass entering the node and not on the value at the previous iteration, we thought to exploit the fact that the pagerank field is still transmitted, and when we are transferring the graph structure we put -1 (impossible value). This allows us to discriminate the situation in which the graph structure is transmitted, in an optimized way (without the need to insert an additional field). When we send the mass instead we put the value to be transmitted as the pagerank value and an empty list as the list of outgoing edges.

\subsubsection{Third Phase: Sorting}
Since the previous phase constructs multiple outputs, also this phase has to take multiple inputs. Here a single reducer is used in order to take advantage of the automatic sorting done over the keys by Hadoop. Keys are in this case the pagerank value of each page, and are passed by the mapper to the reducer as \textbf{DoubleWritable} objects. We have implemented a \textbf{WritableComparator} for getting the descending order.

\subsection{Performance}
For measuring the performance we considered three files with a different number of nodes:
\begin{itemize}
	\item \textbf{wiki-micro.txt}: number of nodes 2427. Most of the nodes points to sites that are not present in the initial set.
	\item \textbf{dataset5.txt}: number of nodes 5000. This dataset is synthetic dataset created using the same structure of \textit{wiki-micro.txt}. Each node has a random value of outgoing edges between 0 and 10.
	\item \textbf{dataset10.txt}: number of nodes 10000. This dataset is synthetic dataset created using the same structure of \textit{wiki-micro.txt}. Each node has a random value of outgoing edges between 0 and 10.
\end{itemize}
and we test them over different numbers of iteration: 5, 10, 15.

\noindent In order to prove the performance gain in \textit{In-Mapper Combiner} instead of counting the nodes in a MapReduce, we provide the result of both the approches. Those results are computed as the mean of 5 iterations.

\begin{table}[H]
\caption{Performance with MapReduce Node Counter}
\centering
\begin{tabular}{c c c c}
\hline\hline
File & N Iter 5 & N Iter 10 & N Iter 15 \\ [0.7ex] % inserts table %heading
\hline
wiki-micro.txt&184417&314290&436651\\
dataset5.txt&186320&310445&467191 \\
dataset10.txt&206945&339101&455945 \\ [1ex]
\hline
\end{tabular}
\label{table:nonlin}
\end{table}

\begin{table}[H]
\caption{Performance with counting in Parsing}
\centering
\begin{tabular}{c c c c}
\hline\hline
File & N Iter 5 & N Iter 10 & N Iter 15 \\ [0.7ex] % inserts table %heading
\hline
wiki-micro.txt&164519&295216&419528\\
dataset5.txt&168450&331793&430656 \\
dataset10.txt&172402&294239&437840 \\ [1ex]
\hline
\end{tabular}
\label{table:nonlin}
\end{table}

\noindent The values in the \textbf{tables} are expressed in milliseconds.